{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install keras_nlp"
      ],
      "metadata": {
        "id": "x3Z2FsU2hNlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0mi6u2UgW1_",
        "outputId": "523ecb59-3373-4d87-c929-a408b2eb2101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import keras_nlp\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Npk0ubj2gW2E"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "BATCH_SIZE = 64\n",
        "SEQ_LEN = 128\n",
        "MIN_TRAINING_SEQ_LEN = 450\n",
        "\n",
        "# Model\n",
        "EMBED_DIM = 256\n",
        "FEED_FORWARD_DIM = 256\n",
        "NUM_HEADS = 3\n",
        "NUM_LAYERS = 2\n",
        "VOCAB_SIZE = 5000  # Limits parameters in model.\n",
        "\n",
        "# Training\n",
        "EPOCHS = 6\n",
        "\n",
        "# Inference\n",
        "NUM_TOKENS_TO_GENERATE = 80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEwfU3z9gW2I",
        "outputId": "41fbd3c0-e941-41f4-958c-0b3e0ed2a737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\n",
            "282386239/282386239 [==============================] - 24s 0us/step\n"
          ]
        }
      ],
      "source": [
        "keras.utils.get_file(\n",
        "    origin=\"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "dir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\")\n",
        "\n",
        "# Load simplebooks-92 train set and filter out short lines.\n",
        "raw_train_ds = (\n",
        "    tf.data.TextLineDataset(dir + \"simplebooks-92-raw/train.txt\")\n",
        "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(buffer_size=256)\n",
        ")\n",
        "\n",
        "# Load simplebooks-92 validation set and filter out short lines.\n",
        "raw_val_ds = (\n",
        "    tf.data.TextLineDataset(dir + \"simplebooks-92-raw/valid.txt\")\n",
        "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "seHK_DM1gW2L"
      },
      "outputs": [],
      "source": [
        "# Train tokenizer vocabulary\n",
        "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
        "    raw_train_ds,\n",
        "    vocabulary_size=VOCAB_SIZE,\n",
        "    lowercase=True,\n",
        "    reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8qkW99W6gW2N"
      },
      "outputs": [],
      "source": [
        "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab,\n",
        "    sequence_length=SEQ_LEN,\n",
        "    lowercase=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lUFP3wQPgW2O"
      },
      "outputs": [],
      "source": [
        "# packer adds a start token\n",
        "start_packer = keras_nlp.layers.StartEndPacker(\n",
        "    sequence_length=SEQ_LEN,\n",
        "    start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
        ")\n",
        "\n",
        "\n",
        "def preprocess(inputs):\n",
        "    outputs = tokenizer(inputs)\n",
        "    features = start_packer(outputs)\n",
        "    labels = outputs\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "# Tokenize and split into train and label sequences.\n",
        "train_ds = raw_train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
        "    tf.data.AUTOTUNE\n",
        ")\n",
        "val_ds = raw_val_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
        "    tf.data.AUTOTUNE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "S1o-r7ccgW2Q"
      },
      "outputs": [],
      "source": [
        "inputs = keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
        "# Embedding.\n",
        "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=VOCAB_SIZE,\n",
        "    sequence_length=SEQ_LEN,\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    mask_zero=True,\n",
        ")\n",
        "x = embedding_layer(inputs)\n",
        "# Transformer decoders.\n",
        "for _ in range(NUM_LAYERS):\n",
        "    decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
        "        num_heads=NUM_HEADS,\n",
        "        intermediate_dim=FEED_FORWARD_DIM,\n",
        "    )\n",
        "    x = decoder_layer(x)  # Giving one argument only skips cross-attention.\n",
        "# Output.\n",
        "outputs = keras.layers.Dense(VOCAB_SIZE)(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
        "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7ZSRtqIgW2R",
        "outputId": "317d262f-ad44-4882-ba88-4472ca03a1aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " token_and_position_embeddi  (None, None, 256)         1312768   \n",
            " ng (TokenAndPositionEmbedd                                      \n",
            " ing)                                                            \n",
            "                                                                 \n",
            " transformer_decoder (Trans  (None, None, 256)         394749    \n",
            " formerDecoder)                                                  \n",
            "                                                                 \n",
            " transformer_decoder_1 (Tra  (None, None, 256)         394749    \n",
            " nsformerDecoder)                                                \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, None, 5000)        1285000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3387266 (12.92 MB)\n",
            "Trainable params: 3387266 (12.92 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyXNqNLEgW2S",
        "outputId": "d52023a2-227e-4e94-ab8e-7f2f2d15f776"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "3169/3169 - 432s - loss: 4.5635 - perplexity: 96.2995 - val_loss: 4.1293 - val_perplexity: 62.8110 - 432s/epoch - 136ms/step\n",
            "Epoch 2/6\n",
            "3169/3169 - 251s - loss: 4.0547 - perplexity: 57.8930 - val_loss: 3.9864 - val_perplexity: 54.3020 - 251s/epoch - 79ms/step\n",
            "Epoch 3/6\n",
            "3169/3169 - 248s - loss: 3.9415 - perplexity: 51.6923 - val_loss: 3.9496 - val_perplexity: 52.3990 - 248s/epoch - 78ms/step\n",
            "Epoch 4/6\n",
            "3169/3169 - 247s - loss: 3.8794 - perplexity: 48.5792 - val_loss: 3.8851 - val_perplexity: 49.0976 - 247s/epoch - 78ms/step\n",
            "Epoch 5/6\n",
            "3169/3169 - 249s - loss: 3.8345 - perplexity: 46.4447 - val_loss: 3.8583 - val_perplexity: 47.9225 - 249s/epoch - 78ms/step\n",
            "Epoch 6/6\n",
            "3169/3169 - 247s - loss: 3.8013 - perplexity: 44.9276 - val_loss: 3.8355 - val_perplexity: 46.7612 - 247s/epoch - 78ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c7bca940d30>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "model.fit(train_ds, validation_datasss=val_ds, verbose=1, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PMWb1_JgW2T",
        "outputId": "8652446e-94ac-441f-d71c-e1bc17d99306"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "      dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# The \"packer\" layers adds the [BOS] token for us.\n",
        "prompt_tokens = start_packer(tokenizer([\"\"]))\n",
        "prompt_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1w447aLzgW2V"
      },
      "outputs": [],
      "source": [
        "\n",
        "def next(prompt, cache, index):\n",
        "    logits = model(prompt)[:, index - 1, :]\n",
        "    # Ignore hidden states for now; only needed for contrastive search.\n",
        "    hidden_states = None\n",
        "    return logits, hidden_states, cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b4Y93R9gW2W",
        "outputId": "0332db14-b8c2-489f-a38e-b6c8f1508b7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy search generated text: \n",
            "[b'[BOS] \" i have been thinking about the matter over , \" he said , \" i have been thinking of the matter over . i have been thinking about the matter over , and i have been thinking that i have been thinking of the matter . i have been thinking about the matter over , and i have been thinking that i have been thinking of the matter . i have been thinking of the matter over , and i have been thinking of the matter . i have been thinking of the matter over , and i have been thinking about the matter over . i have been thinking about the matter over . i have been thinking about the matter over . i have']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sampler = keras_nlp.samplers.GreedySampler()\n",
        "output_tokens = sampler(\n",
        "    next=next,\n",
        "    prompt=prompt_tokens,\n",
        "    index=1,  # Start sampling immediately after the [BOS] token.\n",
        ")\n",
        "txt = tokenizer.detokenize(output_tokens)\n",
        "print(f\"Greedy search generated text: \\n{txt}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2AzVctJgW2Y",
        "outputId": "a05cef2c-d32d-47b8-a8f1-67f637639cc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam search generated text: \n",
            "[b'[BOS] \" well , \" he said , \" i don \\' t know , but i don \\' t know anything about it . i don \\' t know anything about it . i don \\' t know anything about it . i don \\' t know anything about it . i don \\' t know anything about it . i don \\' t know anything about it . i don \\' t know anything about it . i don \\' t know anything about it . i don \\' t know anything about it . i don \\' t know anything about it . i don \\' t know anything about it . i don \\' t know anything about it . i don \\' t']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sampler = keras_nlp.samplers.BeamSampler(num_beams=10)\n",
        "output_tokens = sampler(\n",
        "    next=next,\n",
        "    prompt=prompt_tokens,\n",
        "    index=1,\n",
        ")\n",
        "txt = tokenizer.detokenize(output_tokens)\n",
        "print(f\"Beam search generated text: \\n{txt}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83Hir6tPgW2Z",
        "outputId": "7d203714-d795-48fa-8a15-fe2f117bd1e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random search generated text: \n",
            "[b'[BOS] the bow feather , his stomach was no longer exposed to the precinctiest of notice . puzier was as a lagdiot . but the black creature was the only chance ; great claws being caught by the bows and claws taken from toadstool , that her crying , but gradually , except a fright which was instantly hurled against the stick . a strong pole is like a great run overhead , and the greater portion of her limbs pushed off again . it was only a good swim . hardy was green , the hair man very bare , and he was likely to grow hoisterous . [PAD] therefore he was']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sampler = keras_nlp.samplers.RandomSampler()\n",
        "output_tokens = sampler(\n",
        "    next=next,\n",
        "    prompt=prompt_tokens,\n",
        "    index=1,\n",
        ")\n",
        "txt = tokenizer.detokenize(output_tokens)\n",
        "print(f\"Random search generated text: \\n{txt}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCjChlGogW2b",
        "outputId": "f429f77c-d90a-414f-91c9-04bb016dc2e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-K search generated text: \n",
            "[b'[BOS] the men had already told the story of a man . the man who had been in this room , had told him to give him a great deal of time to learn the business of the men , and then he was sure that they were to learn what he wanted . the people had told them they had been so long in the forest , that he would have done something very well and could not be done to the end ; they told him that the men would be killed by the spaniards ; they would be killed , and the men would be killed . [PAD] also the people said that they would come back from their homes , and would be destroyed']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sampler = keras_nlp.samplers.TopKSampler(k=10)\n",
        "output_tokens = sampler(\n",
        "    next=next,\n",
        "    prompt=prompt_tokens,\n",
        "    index=1,\n",
        ")\n",
        "txt = tokenizer.detokenize(output_tokens)\n",
        "print(f\"Top-K search generated text: \\n{txt}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNTALHsFgW2c",
        "outputId": "f364402a-a29f-48fc-a8e6-b391b722f2d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-P search generated text: \n",
            "[b'[BOS] \" my dear sir , \" he said , \" i have no fear of your being associated , and i have not been able to see any other knight of the court . i have seen that , at least , and were killed , i have heard of the king \\' s senor , but i have heard the king \\' s words of my brother , i have seen him , and that , if he were here , i should have said , that i have been a very strong , strong , noble , and well , i should be glad to see him , as i have been ; but i will have been here']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sampler = keras_nlp.samplers.TopPSampler(p=0.5)\n",
        "output_tokens = sampler(\n",
        "    next=next,\n",
        "    prompt=prompt_tokens,\n",
        "    index=1,\n",
        ")\n",
        "txt = tokenizer.detokenize(output_tokens)\n",
        "print(f\"Top-P search generated text: \\n{txt}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utycnkKegW2f",
        "outputId": "fc2001f2-73f3-4477-c77b-926532a060e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "[b'[BOS] the men were at work in a position of extreme age , who were to be found in the neighbourhood of the combination of the preserver in the country ; and the men were to be taken in the rear of their own . the men , however , was a large body of french cavalry , and were therefore ordered for the spaniards to enter the town ; and as soon as possible , the spaniards , as to their wives ; and men , in the neighbourhood of the village , were to retire ; while a party of natives , to a considerable distance , and to make a great resistance , to the spaniards , the portuguese']\n",
            "\n",
            "1/1 - 14s - loss: 3.7706 - perplexity: 43.4882 - 14s/epoch - 14s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "[b'[BOS] when the king saw that his majesty had been engaged in a great effort to escape , and then went to the palace of his brother . he took the greatest pains and , as soon as the princess got his arms to the palace , he gave her her permission to take his place in safety , with the princess , a servant dressed in his clothes , and , with her jewels , were taken prisoners . the prince and princess prince had received a large supply of presents , and so as to be taken prisoners , to the king . he had been taken prisoner , and was at once received with great loss ; but the king , the prince , with']\n",
            "\n",
            "1/1 - 14s - loss: 3.9368 - perplexity: 51.3757 - 14s/epoch - 14s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c7bdb9b9e10>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "\n",
        "class TopKTextGenerator(keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate text from a trained model using top-k.\"\"\"\n",
        "\n",
        "    def __init__(self, k):\n",
        "        self.sampler = keras_nlp.samplers.TopKSampler(k)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        output_tokens = self.sampler(\n",
        "            next=next,\n",
        "            prompt=prompt_tokens,\n",
        "            index=1,\n",
        "        )\n",
        "        txt = tokenizer.detokenize(output_tokens)\n",
        "        print(f\"Top-K search generated text: \\n{txt}\\n\")\n",
        "\n",
        "\n",
        "text_generation_callback = TopKTextGenerator(k=10)\n",
        "# Dummy training loop to demonstrate callback.\n",
        "model.fit(train_ds.take(1), verbose=2, epochs=2, callbacks=[text_generation_callback])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_generation_gpt",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}